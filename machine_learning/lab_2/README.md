# Отчёт по лабораторной работе: Байесовская классификация


1. Цель работы

> Реализовать и протестировать байесовский классификатор (на основе алгоритма GaussianNB)
> на датасете Iris из репозитория UCI ML.

2. Теоретическая часть
    - Принцип работы байесовской классификации
      Байесовская классификация основана на теореме Байеса:

      P(C|X) = P(X|C) * P(C) / P(X)

      где:
        - P(C∣X) — вероятность класса C при заданных признаках X (апостериорная вероятность); 
        - P(X∣C) — вероятность признаков X при классе C (правдоподобие);
        - P(C) — априорная вероятность класса C;
        - P(X) — вероятность наблюдаемых признаков.
    
    - В реализации GaussianNB предполагается, что признаки в каждом классе распределены по нормальному (Гауссову) закону

3. Методика эксперимента
   - Источник данных
     Датасет Iris взят из UCI Machine Learning Repository:
     https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data
   
   - Описание датасета
     - Количество экземпляров: 150 (по 50 на класс).
     - Признаки (все — числовые, в см):
       sepal_length — длина чашелистика;
       sepal_width — ширина чашелистика;
       petal_length — длина лепестка;
       petal_width — ширина лепестка. 

    - Классы (целевая переменная):
      Iris‑setosa;
      Iris‑versicolor;
      Iris‑virginica.

    - Предобработка данных
      Загрузка данных через pandas.read_csv().
      Разделение на признаки (X) и метки (y).
      Разделение на обучающую и тестовую выборки (70 % / 30 %) с параметром stratify=y для сохранения баланса классов.

    - Параметры модели
      Алгоритм: GaussianNB() из sklearn.naive_bayes.
      Без дополнительной настройки гиперпараметров (значения по умолчанию).

4. Результаты

```
Класс	Precision	Recall	F1‑score	Support
Iris‑setosa	1,00	1,00	1,00	15
Iris‑versicolor	0,82	0,93	0,88	15
Iris‑virginica	0,92	0,80	0,86	15
``` 
- Средние значения:

```
macro avg: 0,92/0,91/0,91;
weighted avg: 0,92/0,91/0,91
```

> Матрица ошибок
> Визуализирована с помощью seaborn.heatmap()

> Визуализация
> Построены графики:
  - истинные классы (по признакам длина лепестка и ширина лепестка);
  - предсказанные классы (аналогичные признаки).

5. Анализ результатов

```
Iris‑setosa классифицируется идеально (precision = recall = 1,00):

Класс хорошо отделяется от остальных по признакам лепестков.

Iris‑versicolor:

Высокий recall (93 %) — модель находит почти все экземпляры класса.

Низкий precision (82 %) — 18 % предсказаний versicolor ошибочны.

Проблема: ложные срабатывания (модель путает с virginica).

Iris‑virginica:

Высокий precision (92 %) — среди предсказаний virginica лишь 8 % ошибок.

Низкий recall (80 %) — 20 % экземпляров virginica пропускаются.

Проблема: пропуски (модель относит к versicolor).

Основная причина ошибок: перекрытие признаков между versicolor и virginica, особенно по длине и ширине лепестков.
```

6. Выводы

> Модель демонстрирует хорошую общую точность (91,11 %), что подтверждает применимость GaussianNB для задачи классификации Iris.

> Наибольшие трудности возникают при разделении versicolor и virginica из‑за схожести их характеристик.


## Ответы на контрольные вопросы

1. Почему байесовский алгоритм называют наивным?

> Алгоритм называется «наивным» из‑за ключевого допущения о независимости признаков. 
> В реальности признаки часто коррелируют друг с другом (например, длина и ширина лепестка у растений),
>  но модель игнорирует эти взаимосвязи и считает, что каждый признак вносит независимый вклад в вероятность класса.
> Это упрощение делает вычисления управляемыми, но редко соответствует действительности — отсюда термин «наивный».

2. Чем статистический подход к классификации отличается от детерминистского?
> Статистический подход (как байесовский):
> Опирается на вероятностные модели и распределения данных.
> Учитывает неопределённость: выдаёт не только метку класса, но и вероятность принадлежности к нему.
> Обучается на статистических закономерностях в данных (например, оценивает P(класс∣признаки)).

> Детерминистский подход (например, решающие деревья, SVM):
> Строит жёсткие правила или границы разделения без явного моделирования вероятностей.
> Считает, что при заданных признаках класс определяется однозначно.
> Не даёт вероятностной интерпретации результатов.

3. Приведите теорему Байеса, используемую для классификации.
> Для классификации применяется формула:

```
P(Ck|x) = P(x|Ck) * P(Ck) / P(X)

где:

P(Ck∣x) — апостериорная вероятность класса Ck при заданных признаках x (искомая величина);

P(x∣Ck) — правдоподобие (вероятность наблюдать признаки x для класса Ck);

P(Ck) — априорная вероятность класса Ck (до наблюдения признаков);

P(x) — вероятность наблюдения признаков x (обычно опускается при сравнении классов).
```

4. Что такое правдоподобие?

> Правдоподобие (P(x∣Ck)) — это вероятность того, что объект с признаками x принадлежит классу Ck.
> В байесовской классификации оно моделируется через распределение признаков внутри каждого класса
> (например, нормальное распределение для GaussianNB). Чем выше правдоподобие, тем «типичнее» объект для данного класса.


5. Каким в самом простом случае при построении байесовских классификаторов является представление плотностей распределений условных вероятностей образов классов?
> В простейшем случае (например, для GaussianNB) предполагается, что признаки внутри каждого класса распределены нормально (по Гауссу).

6. Перечислите преимущества и недостатки Наивного байесовского алгоритма (НБА).
> Преимущества:
  - Простота реализации — минимальные вычислительные затраты на обучение и предсказание.
  - Высокая скорость — подходит для потоковой обработки данных.
  - Эффективность при малых объёмах данных — даёт приемлемые результаты даже на небольших выборках.
  - Устойчивость к нерелевантным признакам — за счёт вероятностной природы.
  - Интерпретируемость — можно анализировать вклад каждого признака через правдоподобие.
  - Работа с категориальными данными — версии вроде MultinomialNB оптимизированы для текстовых задач.

 > Недостатки:
   - Допущение о независимости признаков — часто нарушается в реальных данных, что снижает точность.
   - Проблема «нулевой вероятности» — если признак не встречался в обучающей выборке для класса, 
     правдоподобие становится нулевым (решается сглаживанием, например, additive smoothing).
   - Чувствительность к масштабированию — требует нормализации признаков для корректной работы с разными масштабами.
   - Предположение о нормальности (для GaussianNB) — если распределение признаков сильно отличается от Гаусса, качество падает.
   - Смещение в пользу крупных классов — если классы несбалансированы, модель может игнорировать малые классы.
   - Ограниченная гибкость — не способен моделировать сложные нелинейные зависимости без дополнительных преобразований.