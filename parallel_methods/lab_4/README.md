# Лабораторная работа №4  
## Курс «Параллельные методы и алгоритмы»

### Тема
Распределённые вычисления MapReduce

---

## Цель работы
Научиться писать и отлаживать распределённые приложения на основе методологии **MapReduce**, а также освоить базовые принципы обработки текстовых данных в распределённой среде.

---

## Задание
Разработать MapReduce-приложение для подсчёта количества **уникальных слов** во входном текстовом файле.

---

## Описание решения

Решение состоит из трёх Python-скриптов:

### 1. mapper.py
Mapper считывает входной текст построчно, приводит его к нижнему регистру, разбивает строки на слова и формирует пары вида:


Каждое слово передаётся как ключ, а значение `1` используется для дальнейшей обработки.

---

### 2. reducer.py
Reducer получает отсортированные пары «ключ–значение» и определяет уникальные слова.  
Для каждого нового слова выводится одна строка, что позволяет исключить повторяющиеся ключи.

Формат вывода: слово 1


---

### 3. count_unique.py
Данный скрипт подсчитывает количество строк, поступающих на вход, что соответствует количеству уникальных слов после этапа Reduce.

Результат выводится в формате: Unique words: N


---

## Запуск программы

Пример запуска с использованием стандартного ввода и вывода:

```bash
cat text.txt | python mapper.py | sort | python reducer.py | python count_unique.py
```

Контрольные вопросы
1. В чем разница между вертикальным и горизонтальным масштабированием системы?

Вертикальное масштабирование заключается в увеличении ресурсов одного узла, а горизонтальное — в добавлении новых узлов в систему и распределении нагрузки между ними.

2. Раскройте суть подхода MapReduce.

MapReduce — это модель распределённых вычислений, состоящая из этапов Map (формирование пар «ключ–значение») и Reduce (агрегация данных по ключу).

3. Приведите примеры задач, где не нужна процедура Reduce.

Фильтрация данных, преобразование форматов, нормализация данных, извлечение информации без агрегации.

4. Кратко опишите основные паттерны MapReduce.

Word Count, Filtering, Sorting, Grouping, Join, Summarization.

5. Перечислите основные компоненты фреймворка Hadoop.

HDFS, YARN, MapReduce, Hadoop Common.

6. Что представляет собой файловая система HDFS?

HDFS — распределённая файловая система для хранения больших объёмов данных с репликацией и высокой отказоустойчивостью.

7. Как работать со streaming-интерфейсом Hadoop?

Hadoop Streaming позволяет использовать программы на любых языках в качестве mapper и reducer через стандартные потоки ввода и вывода.

8. Кратко опишите назначение фреймворка Spark.

Apache Spark — фреймворк для быстрой распределённой обработки данных в памяти, поддерживающий аналитические и потоковые вычисления.

9. Кратко опишите назначение фреймворка Hive.

Apache Hive предоставляет SQL-подобный интерфейс для анализа данных, хранящихся в Hadoop.

10. Как реализует подход MapReduce библиотека mrjob?

Библиотека mrjob упрощает написание MapReduce-программ на Python и позволяет запускать их локально или в Hadoop-среде.

Вывод

В ходе лабораторной работы было реализовано MapReduce-приложение для подсчёта количества уникальных слов в тексте. Были изучены принципы распределённой обработки данных и получены практические навыки работы с mapper и reducer-программами.