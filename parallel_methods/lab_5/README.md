# Отчёт по лабораторной работе: «Распределённые вычисления с Apache Spark»


## 1. Цель работы

Освоить основы разработки распределённых приложений на Apache Spark (PySpark):
- изучить архитектуру и ключевые концепции Spark;
- реализовать приложение для подсчёта уникальных слов в тексте;
- проанализировать результаты и ответить на контрольные вопросы.

## 2. Выполненные задачи

### 2.1. Настройка окружения
- Установлен Python 3.x, Java 8+, PySpark.
- Проверена работоспособность окружения через `spark-submit`.

### 2.2. Разработка приложения
- Создан скрипт `word_count_pyspark.py`.
- Реализована загрузка текста из файла.
- Выполнена очистка данных:
  - удаление знаков препинания;
  - приведение к нижнему регистру.
- Подсчитано количество уникальных слов через RDD‑операции.
- Результат сохранён в выходной файл.

### 2.3. Тестирование
- Использован тестовый файл `input.txt` (~3 000 слов).
- Получено **716 уникальных слов**.
- Проверены предупреждения Spark и предложены способы их устранения.

### 2.4. Анализ результатов
- Сопоставлены результаты PySpark и чистой Python‑реализации.
- Рассмотрены варианты оптимизации:
  - переход на DataFrame API;
  - кэширование данных.

## 3. Ответы на контрольные вопросы

### 3.1. Разница между пакетной и потоковой обработкой в Spark

**Пакетная обработка** (`Spark Core`, `Spark SQL`):
- данные обрабатываются блоками (RDD/DataFrame);
- операции выполняются после вызова действия (`count()`, `save()`);
- подходит для анализа исторических данных.

**Потоковая обработка** (`Spark Streaming`, `Structured Streaming`):
- данные поступают непрерывно (например, из Kafka);
- обрабатываются микро‑пакетами с заданным интервалом;
- используется для мониторинга в реальном времени.

### 3.2. Достоинства Spark
- **Скорость**: обработка в памяти (в 10–100 раз быстрее Hadoop MapReduce).
- **Универсальность**: поддержка SQL, машинного обучения (MLlib), графов (GraphX), потоковой обработки.
- **Простота API**: удобные абстракции (RDD, DataFrame, Dataset) на Python, Scala, Java, R.
- **Отказоустойчивость**: автоматическое восстановление данных через lineage‑графики.
- **Интеграция**: работа с HDFS, S3, Kafka, Cassandra и др.

### 3.3. Недостатки Spark
- **Потребление памяти**: требует значительных ресурсов ОЗУ для кэширования данных.
- **Сложность настройки**: оптимизация параметров (`spark.executor.memory`, `spark.sql.shuffle.partitions`) требует экспертизы.
- **Задержки в потоковой обработке**: микро‑пакетный подход не обеспечивает истинную low‑latency обработку (как в Flink).
- **Зависимость от JVM**: ограничения при работе с native‑библиотеками.

### 3.4. Функции ядра Spark (Spark Core)
- управление распределёнными данными (RDD);
- планирование задач и распределение нагрузки;
- отказоустойчивость (восстановление данных через lineage);
- API для базовых преобразований (`map()`, `filter()`, `reduce()`);
- интеграция с системами хранения (HDFS, S3).

### 3.5. Дополнительные библиотеки Spark
- **Spark SQL**: работа с структурированными данными, поддержка SQL‑запросов.
- **MLlib**: алгоритмы машинного обучения (классификация, кластеризация, регрессия).
- **GraphX**: анализ графов (PageRank, связные компоненты).
- **Spark Streaming / Structured Streaming**: потоковая обработка данных.
- **Spark R**: интерфейс для языка R.

### 3.6. Отличие Spark от MapReduce
- **Память vs диск**: Spark использует ОЗУ для промежуточных данных, MapReduce записывает на диск после каждой операции.
- **Скорость**: Spark быстрее за счёт ленивых вычислений и оптимизации планов выполнения.
- **API**: Spark предлагает высокоуровневые абстракции (DataFrame), MapReduce требует ручной реализации map/reduce.
- **Экосистема**: Spark включает SQL, ML, streaming; MapReduce ограничен пакетной обработкой.

### 3.7. Назначение RDD в Spark
**RDD** (Resilient Distributed Dataset) — неизменяемая распределённая коллекция объектов.

**Функции**:
- хранение данных на множестве узлов;
- автоматическое восстановление при сбоях (через lineage);
- поддержка преобразований (`map`, `filter`) и действий (`count`, `collect`).

**Пример**: `textFile()` создаёт RDD из файла, `flatMap()` разбивает строки на слова.

### 3.8. Трансформации и действия в Spark

**Трансформации** (`map()`, `filter()`, `join()`):
- ленивые операции, не запускают выполнение;
- возвращают новый RDD/DataFrame;
- строятся в lineage‑график.

**Действия** (`count()`, `saveAsTextFile()`, `collect()`):
- запускают вычисление lineage‑графика;
- возвращают результат или сохраняют данные;
- завершают цепочку трансформаций.


### 3.9. Lazy Evaluation (ленивые вычисления) в Spark

**Суть**: Spark не выполняет трансформации сразу, а строит план (DAG) до вызова действия.

**Механизм**:
1. Пользователь вызывает `map()`, `filter()` — Spark запоминает операции.
2. При вызове `count()` или `save()` — оптимизирует и запускает выполнение.

**Цель**: минимизация операций с диском и пересылок данных.


### 3.10. Достоинства и недостатки Lazy Evaluation

**Достоинства**:
- оптимизация плана выполнения (сокращение shuffle, объединение операций);
- экономия ресурсов (данные не загружаются до необходимости);
- гибкость в построении сложных конвейеров.

**Недостатки**:
- сложность отладки (ошибки проявляются только при вызове действия);
- неочевидные задержки при первом действии;
- риск накопления большого lineage‑графика (проблемы с памятью).


## 4. Выводы

1. Apache Spark — мощный инструмент для распределённой обработки данных, сочетающий скорость, универсальность и отказоустойчивость.
2. В ходе работы освоены:
   - базовые операции с RDD (загрузка, очистка, подсчёт уникальных значений);
   - анализ предупреждений Spark и способы их устранения;
   - сравнение Spark с альтернативными подходами (чистый Python, MapReduce).
3. Для продуктивного использования Spark рекомендуется:
   - переходить на DataFrame API для сложных задач;
   - настраивать параметры памяти и параллелизма;
   - учитывать компромиссы Lazy Evaluation при проектировании пайплайнов.

## 5. Приложения

- **Исходный код**: `word_count_pyspark.py`.
- **Тестовый файл**: `input.txt` (~3 000 слов).

